---
title: "AMP - Designing an RCT "
author: '[Michael Barber](mailto:mike.barber@oneacrefund.org)'
date: "September 4th, 2017"
output:
  html_document:
    code_folding: hide
    number_sections: yes
    theme: flatly
    toc: yes
    toc_depth: 6
    toc_float: yes
  pdf_document:
    latex_engine: xelatex
    toc: yes
    toc_depth: 6
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2) # this is a nice plotting package we will use
library(cowplot) # this gives us extra options when plotting 



```


# AMP lesson: RCT principles


## Lesson objectives:


### Motivation:

The randomized controlled trial (RCT) is a key tool for any analyst, what separates the RCT from other statistical tools is its ability to define causality. Whereas most statistical techniques can only tell us about *correlations*, the RCT can tell us about *causations*, i.e. whether A causes B, or B causes A. As such, the RCT is central to proving what does and does not work for our farmers.

This lesson will outline the design principles of RCTs, you should consult this lesson before every RCT to ensure you have considered each and every aspect of an effective and cost-efficient RCT. Later lessons will deal with how to analyse RCT results. Note that not all trials will be a full-fledged RCT, there will also be simpler trials run in earlier phases to obtain estimates of logistical feasibility and impact (etc). These are described  [here ](https://docs.google.com/presentation/d/14CpKbrEtFOzPfYTmfwKZgnpmANYbKC8YmX1ZxTRZsQs/edit#slide=id.g204cb16ee8_0_0). 

### Objectives:

After this lesson you should know which steps to take when designing an RCT and the order to follow them, you should also be able to judge whether an RCT is feasible given constraints on sample sizes and measurement strategies. 

We will break an RCT into it's parts (hypothesis, randomization, and measurement) and examine what is required from each part to make an effective RCT.

## Glossary and key terms:

* **Intervention**: An intervention is the product or program innovation you are testing in a trial, e.g. repayment incentives, or new seed varieties.

* **Co-variate** : Other variables in a study that may correlate with your outcome. E.g. if you are measuring the effect of solar lighting on farmer repayment then maize yield might be an important co-variate that influences our outcome. 

* **Cluster**: A group of individuals sharing some co-variates. An example at One Acre Fund would be a group or site. 

* **Randomisation unit**: The level at which randomization (and analysis) occurs, this could be the individual, the group, or the site (or other levels)

* **Outliers**: Data points which do not seem to follow the main distribution, note that this is often very subjective.

* **Scientific writing**: A style of writing that makes work transparent and communicable to other analysts

* **Treatment group**: Those individuals or clusters that have been assigned to an intervention

* **Control group**: Those individuals or clusters that have been assigned to not receive an intervention


## Principles of Randomised controlled trials

Each RCT will have some shared elements:

* **Randomization**: The random assignment of individuals or groups to a treatment or control group

* **Hypothesis**: A proposition (or statement) to evaluate.

* **A control group**: a randomly selected group assigned to *not* receive the intervention

* **A treatment group** a randomly selected group assigned to receive the intervention. It is possible to have multiple treatment groups in an RCT, one for each intervention. 

* **Measurement**: The act of recording observations from the outcome variable

* **data checking**: Checking data and distributions during a trial for consistency and quality

These are combined into an RCT. An RCT will allow us to assign causality and determine the impact of an intervention if we hold every other variate constant. Let us examine each of these in turn. 

### Randomisation in RCTs

Why do we randomize in an RCT? And why would we not want farmers to self-select whether they are treatment or control? It essentially comes down to eliminating **bias** and distributing **co-variates** equally. 

Co-variates are factors that might influence your outcome variable, for example, farmer location, farmer income, soil type (etc). Bias, in proper statistics jargon, *means when your sample is substantially different from your popultion*. In an RCT we are assuming our sample is representative of our population, deviations from this assumption can lead us to an incorrect understanding of our population and generating conclusions that look robust but are actually invalid! The two types of bias we encounter at this stage of an RCT are:

* Randomization bias - bias can be due to poor randomization resulting in *unbalanced T/C groups* (e.g. we have more control than treatment farmers in District X). 

* Selection bias - bias would also result if we were to allow farmers to assign themselves to T/C groups. This is because there might be an unobservable reason why farmers are choosing T/C groups. For example, wealthier farmers might have more risk appetite and therefore select themselves into treatment groups more, 

    * Both of these will lead to what is called **confounding bias**, this means it will be difficult to untangle effects that are due to poor randomization and effects that are due to the actual intervention.
    
    
* However, if each participant has an equal chance of being randomly assigned to a T/C group then randomization will be free of bias. This will result in both observable (e.g. farmer location) and un-observable co-variates (such as risk appetite) being spread equally to T and C groups. It is this spreading of co-variates that allows us to understand causality. 

It is therefore important to select treatment and control groups *totally randomly*, the best way to achieve this is by letting R do the work for you, as in the below code:

```{r sampling, comment=NA, warning=FALSE}

#lets first make up a fake list of farmer IDS from 1 to 1000 and 1000 random variables drawn from a normal dist. (called yield)
df = data.frame("OAFID"=seq(1,1000), "yield"=rnorm(1000))

#lets now make a function to do the work - you can copy paste this funciton into your own scripts
# it needs to be given a dataframe and a list of naming options
# Options might be "treatment" and "control", or if there are more than 2 options then it might be "Control", "treatment1", "treatment2", or "control", "sunking home", "sunking pro"
#note you will need to copy this bit directly below for it to work in your code
RCT_random = function(dataframey, values_to_add){
  dataframey$values_to_add[sample(1:nrow(dataframey), nrow(dataframey), FALSE)] <- rep(values_to_add)
  colnames(dataframey)[which(colnames(dataframey)=="values_to_add")] = "Status"
  return(dataframey) }


# We set a seed to make our results replicable. A seed means it will always draw numbers the same way. You can make up any number for the seed (9 is my example)
set.seed(9)
# so this will take the dataframe called "df" and randomly assign each ROW to "Treatment" or "control"
df_new = RCT_random(df, c("Treatment","Control"))

# so this will take the dataframe called "df" and randomly assign each ROW to either "treatment1", "treatment2" or "control"
df_new2 = RCT_random(df, c("Treatment1","Treatment2", "Control"))

```

We have now taken our original dataframe:

```{r showdf, comment=NA}
head(df)
```

And randomly assigned T and C status to them with my function "RCT_random":

```{r showdf2, comment=NA}
head(df_new)
```


We should now double-check our randomization to ensure it has proceeded as expected, to do this let us look at the summary statistics for Treatment and Control groups and make sure the "yield" variable is similar:

```{r hypolooksie}


ggplot(df_new, aes(x=yield, fill=Status)) + geom_density(alpha=.3) + xlab("Yield")



```


They look pretty similar! So far so good, randomization has been successful.

> Let's look at some examples of randomisation strategies below and try to decide whether they are proper or improper randomisation:

1. Farmers are allowed to decide whether to be in a treatment or control group

2. Any farmer with a national ID number ending in an odd number is assigned to treatment, any farmer with an ID ending in an even number is assigned to control.

3. Farmers east of the OAF office are assigned to control, farmers west of the OAF office are assigned to treatment

4. We flip a coin to decide whether a farmer is control or treatment

Which of the above are truly randomized? Take a moment to think about this before expanding the answer below.

<div id="spoiler" style="display:none">


1. Farmers are allowed to decide whether to be in a treatment or control group. <span style="color:red"> This is not random, as farmers may have a reason for choosing a group which would lead to self-selection bias</span>

2.  Any farmer with a national ID number ending in an odd number is assigned to treatment, any farmer with an ID ending in an even number is assigned to control. <span style="color:red"> Almost random, but not quite! The issue here is that there might be some unknown factor associated with national ID that influences our results. If we 100% knew that national IDs were made randomly then this would be fine, however as we cannot know that we should use an alternative method </span>

3. Farmers east of the OAF office are assigned to control, farmers west of the OAF office are assigned to treatment. <span style="color:red">This is a terrible idea, this will likely lead to confounding bias as there are likely to be geographical differences between farmers east and west</span>

4. We flip a coin to decide whether a farmer is control or treatment, Heads means treatment, Tails means Control.<span style="color:blue">This is the only truly random system, as it is based *only* on chance</span>



</div>

<button title="Click to show answer" type="button" onclick="if(document.getElementById('spoiler') .style.display=='none') {document.getElementById('spoiler') .style.display=''}else{document.getElementById('spoiler') .style.display='none'}">Show/hide</button>


> Summary

* **We must properly randomize farmers into treatment and control groups to eliminate confounding bias and ensure that the sample we have (which we actually observe) is representative of the population we care about. Furthermore, we must distribute co-variates equally to be able to draw conclusions about causality.**

* **We achieve proper randomization by each and every farmer (or group or site) having an equal probability of being assigned to a treatment group. The best way to do this is by letting R do the work for you and using my RCT_random function above**. 


### Cluster randomised trials

Often at One Acre Fund we will be running *cluster randomised trials* rather than *randomised controlled trials*. The difference between these two trials lies in the  **unit of inference** and the **unit of randomisation**. A traditional RCT seeks to assign treatment and control status at the individual level, whilst a cluster RCT seeks to assign treatment and control at a cluster level. A **cluster** is any group of individuals with shared characteristics that cannot be randomized away, or where there is significant risk of *contamination* of control/treatment status.

For example, if we were to assign some members of a lending group to treatment and some to control and test the impact of a new training on maize yields then there is a fair chance that communication between members of the same group would mean our control farmers could pick up the new training. This would weaken our ability to understand the impact of the training on yields. We would therefore want to randomize at the group or site level to minimize contamination (so all members of certain groups or sites are assigned T/C status).

A general issue with lending groups at OAF is that farmers self-select into groups, thus generating selection bias. It is therefore rare that we would ever randomize at the individual level. 

When analyzing data from a cluster RCT it is important to summarize the data by cluster. This means we match the unit of randomization to the unit of analysis. For example, if we randomize groups into treatment and control and want to understand the effect of an intervention on maize yields then we would want to summarize the farmers by group, so that we have the group average maize yield. I have included some R code below to show how to do this:

This is our raw data, broken down at the farmer level:

```{r summy, comment=NA}

#make some data
df = data.frame(FarmerID=seq(1,100), Maize_yield=rnorm(100,500,50),  group_name=c("farmers_first","farmers_second","farmers_third","farmers_last"), District=c("A","B"))

library(knitr)
kable(df[1:9,], format="markdown", align="c")



```

In total we have `r dim(df)[1]` rows of data. Now let us summarize it by group_name:

```{r summy2, comment=NA, warning=FALSE, message=FALSE}


library(dplyr) # we need this for the %>% 
# we call %>% "piping"
# we can pipe an object to a new function to simplify work
# so below we are taking df and piping it to a function that will group our data by "group name". Once we have grouped our data we will pipe it again to smmarise_each which will try to calculate the mean of each column in our dataframe. This final object is then saved as sum_df
sum_df = df %>% group_by(group_name) %>% summarise_each(funs(mean))

#kable makes a pretty table in r markdown HTMLs
kable(sum_df, format="markdown", align="c")
```

We now have the average maize yield for each of our four groups ("farmers_first", "farmers_second", "farmers_third", "farmers_last"). Note that we also have the "average" ID number, which is obviously nonsense, also note that our non-numeric columns (like District) have been replaced with NAs (as we cannot average characters). Finally, be aware that if one of our numeric columns were set to class "character" or class "factor" rather than class "numeric" then it would also fail to average them. 


Note that we can also manually manipulate our data if we want to specifically create new metrics:

```{r manip2, warning=FALSE, message=FALSE, comment=NA}

library(plyr)

#this will take df and group it by "group name". It will then create a new metric called "average_yield" which is the mean of maize_yield. Likewise it will create a column called "stdev_yield" that will have our standard deviations
sum_df2 <-  ddply(df, c("group_name"), summarise,  average_yield= mean(Maize_yield), stdev_yield= sd(Maize_yield))

head(sum_df2)
                  
                  


```

This means that the sample size of 100 farmers in 4 groups is not 100! The reason is that under an RCT we assume that each individual has equal information gain, however, if farmer variables are correlated within groups then we already know something about the second farmer in the cluster just by examining the first farmer. This means the second farmer in our group has less information than the first, and so the third, fourth, fifth (etc) will have less information than each farmer preceding it. This drastically reduces our sample size because again, we violate the assumption of equal information for every individual. 



> Let's look at some trial hypotheses and decide what level (farmer, group, FO, district) we should randomise at:

1. Trialing incentives for Field officers to improve farmer repayment

2. Trialing health insurance interventions to see if it improves farmer welfare

3. Trialing new planting practices taught by FOs


Take a moment to think about this before expanding the answer below.

<div id="spoiler2" style="display:none">


1. Trialing incentives for Field officers to improve farmer repayment <span style="color:red"> We would want to trial this at the FO/site level as we are testing an intervention for FOs </span>

2.   Trialing health insurance interventions to see if it improves farmer welfare <span style="color:red"> In theory, this could be trialed at the farmer level as there is no risk of spillover. However, due to morale and logistics this would probably want to be trialed at the site level so that farmers do not feel the trial is unfair </span>

3. Trialing new planting practices taught by FOs <span style="color:red"> Again, this would want to be randomized at the FO level as it would be the FOs doing the instruction  </span>

</div>

<button title="Click to show answer" type="button" onclick="if(document.getElementById('spoiler2') .style.display=='none') {document.getElementById('spoiler2') .style.display=''}else{document.getElementById('spoiler2') .style.display='none'}">Show/hide</button>



If necessary, we can directly calculate the ICC value for a study to determine where to randomize, remember our un-summarized dataframe from earlier:

```{r icyy0}

kable(head(df))
```

We can calculate the ICC using the snippet of code below: 
```{r iccy, comment=NA}

library(ICC)


# this is my function. It will calculate the confidence intervals for ICC
#it needs two character strings, x and y, which are the group level and the outcome variable column names in ""
# it also needs the dataframe which has column names x y
ICC_CI <- function(x,y, dataf){
  si = round(dim(dataf)[1]*0.66)
  values_f <- c()
  for(i in seq(1:300)){
  
  samp_f = dataf[sample(nrow(dataf), si), ]
  
  x_f = ICCbare(x,y,samp_f)
  
  values_f <- c(values_f, x_f)
  }
  # note that 1.96StDevs = 95% confidence interval bounds in a normal dist.
   ret = data.frame("Mean ICC" = round(mean(values_f, na.rm=TRUE),3), "CI" = round(1.96*sd(values_f, na.rm=TRUE),3))
   
   ret$Significant = ifelse(ret$Mean.ICC > ret$CI, "Y", "N")
  return( ret)
  
  }
#using kable to make the dataframe that is returned look pretty
ICC_CI("group_name", "Maize_yield", df)


```

We can see from this calculation that our ICC between farmers in the same lending group is `r round(ICC_CI("group_name", "Maize_yield", df)$Mean.ICC,3)` +- `r round(ICC_CI("group_name", "Maize_yield", df)$CI,3)`. As the confidence intervals (CI) cross zero we can see that this is not significant (hence "Significant" is "N"). This means that randomizing at the group level is fine. Had our ICC been significant ("Significant" = "Y") then we would want to repeat the calculation at the site level. 

In practice, at One Acre Fund we tend to assume an ICC of 1.0 and use the number of clusters (groups or sites) as the sample size (i.e. we calculate the sample size for the number of groups/sites needed, not the number of farmers). This is often the safest and easiest way to proceed. Using this method, our sample size for the above example would be only 4 (instead of 100). Sample size calculation methods with a determined ICC can be found in the upcoming FAQ. 

> Summary:

* **Cluster-level effects can compromise an RCT. It is important to randomize and analyze at the correct level!**

* **You can estimate the ICC using historical data and my function to find the level at which ICC is lowest.**

* **It is likely that almost all studies will be done at the group or FO level.** 


### The Hypothesis

Once we know how to randomize our clusters, we need a strong hypothesis to test. We will be testing our hypothesis by comparing our treatment and control groups. Each hypothesis has two parts:

* Null hypothesis (H0) : The null hypothesis usually states that there is *no difference* between treatment and control groups

* Alternative hypothesis (H1): The alternative hypothesis states that *there is a difference* between treatment and control groups. 

To give an example:

* Null hypothesis: Farmer groups who adopt solar will not have a higher final repayment percentage. 

* Alternative hypothesis: Farmer groups who adopt solar will have a higher final repayment percentage. 


To test this, we will randomly assign some farmers to get solar, and some farmers to not get solar, and then measure the final percentage of loans repaid in each group. However, there are a number of ways we can test for differences between treatment and control groups, we could look for differences in group average repayment, median repayment, or maximum repayment. A good hypothesis, therefore, will state what difference we are looking for (note that this hypothesis will need to be paired to an appropriate hypothesis test - lesson 3). 

An example of a good hypothesis:

* **H0: Groups with a higher adoption percentage of solar lights will have no difference in average percentage repaid by the end of the season**

* **H1: Groups with a higher adoption percentage of solar lights will have a higher average percentage repaid at the end of the season**

Note that it is a clear and simple hypothesis, it is easily testable (by distributing solar lights randomly) and specifies *how* we will test for differences (group level average repayment). 

A bad hypothesis example would be be:

* **H0: Ladybugs are a not good natural pesticide for treating aphid infected plants **

* **H1: Ladybugs are a good natural pesticide for treating aphid infected plants **

Why is this so bad? Take a moment to think before expanding the answer below:

<div id="spoiler3" style="display:none">

* There is no clear definition of "good natural pesticide" what does being a good pesticide mean? We can imagine a pesticide that destroys 100% of pests but also 100% of crops. Is this a good pesticide?

* How would we randomize this? It is difficult to randomly distribute ladybugs in one area, and to prevent ladybugs being in another area!

* There is no clear way to measure this!


</div>

<button title="Click to show answer" type="button" onclick="if(document.getElementById('spoiler3') .style.display=='none') {document.getElementById('spoiler3') .style.display=''}else{document.getElementById('spoiler3') .style.display='none'}">Show/hide</button>

A final example of a bad hypothesis and bad thought pattern is the "untestable hypothesis". This is a hypothesis that cannot be disproved, or that constantly changes to have new criteria. A common example of this at One Acre Fund is:

* **There was bad rainfall in 2016 which caused poor farmer repayment through expected psychological stress**

Note that the way this is phrased means it is impossible to evaluate scientifically:

* Bad rainfall is a poorly defined term and it allows switching of definitions to maintain the hypothesis if presented with contradictory evidence

* We cannot observe a farmer's expected stress in 2016, which means we have no way of collecting evidence to prove or disprove this hypothesis. 

Hopefully it is clear that this hypothesis is untestable, and therefore useless for improving our program. 


Once we have a testable hypothesis and a metric (see below), we will use a hypothesis test to detect differences in the underlying populations represented by our control and treatment samples [AMP lesson 1](https://michael-bar.github.io/AMP-1-distributions/AMP_distributions_lesson.html#). A hypothesis test will yield a P-value, which is the probability that the populations behind our samples are different. When using a hypothesis test we must set an acceptable level of risk. The acceptable level of risk is known as the p-value threshold, or the alpha level. The most common p-value threshold is 0.05. This means that we are willing to accept a 5% risk of rejecting the null hypothesis incorrectly. In other words, 5% of the time we will conclude there is no difference between our treatments when in fact there is. 

In some cases we might want to set a threshold of 0.01 (1%) or 0.1 (10%). Generally speaking, the more unwilling we are to be incorrect, the lower the threshold. So for an intervention that might have adverse effects on farmers, we would want to be very sure of the positive effects (0.01 threshold) and unwilling to accept negative effects (0.1 threshold). Note that the lower our threshold the larger the sample size needed to detect any effect. 

The principles of power and sample size, that we covered in [AMP lesson 1](https://michael-bar.github.io/AMP-1-distributions/AMP_distributions_lesson.html#), are *incredibly* important here. Low power or sample size will mean that we have very little chance of actually detecting an effect whether it exists or not, furthermore, with low power we will not be able to trust any significant results we do find. Remember that power and sample size calculations require an estimate of the effect size, we can usually obtain an estimate from historical data (what did a regression analysis of historical data suggest the impact on repayment of solar ownership was?) or from literature (ideally peer reviewed sources). 

It is possible to skip a hypothesis test and proceed directly to regression results (lesson 3). However it is a good sanity check to make sure that our regression results match our hypothesis test in an RCT. If these results do not match then it suggests that our RCT was designed poorly. 


> Summary

* An RCT requires a clear and well defined hypothesis that is testable. Often a hypothesis will start with a question ("What does solar do to repayment?"), it is the role of the analyst to convert this question into a robust hypothesis with an agreed method of testing. 

* Once we have a hypothesis we need an estimate of the effect size we expect to see, this estimate is directly relevant to sample size calculations (remember a small sample size that is too small makes your results unreliable!). Our estimate can come from historical One Acre Fund data or from well-regarded literature (e.g. from peer-reviewed publications). 

* We must also set a threshold for our P-value. This threshold represents the level of risk we are willing to accept in being wrong. Note that being more stringent (i.e. lower thresholds) will require a larger sample size!

### Measurements

The final part of the RCT is the measurement itself. This is an often neglected part of RCT design and so usually the source of most problems later on. A hypothesis needs to have a good measurement strategy to be useful. 

I want to take a tangent to describe some analysis I did on an RCT looking at airtime usage in Malawi for a telecommunication company. The RCT examined the effect of an intervention on airtime usage. During the RCT, we asked people to recall how much money they had spent in the last month on airtime, we also had data from the telecommunications company on the *actual* amount spent by the same customers. When we compared the two metrics, there was very little correlation! Furthermore, when we looked at who was most likely to over-estimate their airtime spend, we found it was young, urban males.

Now consider -  had we only had the self-reported data, we would have thought that young urban men were big spenders on airtime and drawn many conclusions from this and found many "statistically significant" (in terms of P-values of < 0.05) relationships! We would have made many recommendations to the telecommunications company and all our data would have held up to statistical interrogation. In short - there would have been very little way to know we were wrong!

The moral of this story is that the best statistics in the world will not save a trial from poor measurement. The other moral is that self-reported data is often terrible, as it is influenced by how people want to be perceived by the interviewer or by their community, or by embarrassment about "low" airtime spends. 

Now, if we were trying to understand the effect of increased yield on farmer food spending, how might we go about it?

Our first ideas might be:

* Weekly enumerators visits to trial participants to ask them to recall their last weeks spend on food.

* Daily spend diary kept by the farmers outlining where they spend their money every day. We then collect the diary after the trial



But both these ideas might suffer from the same bias as my airtime study anecdote. This kind of bias is known as **measurement bias** and might arise because some farmers are more forgetful than others, or because some farmers feel social pressure to inflate/deflate their numbers. In this case it requires some careful thought about how to obtain meaningful data. 

How would you improve this trial so the data was more robust (feel free to email ideas to me at Mike.Barber@)?

My thoughts so far:

* We could try to get mobile money data from the farmer or from a third-party (in countries where mobile money is common)

* We could have the enumerator visit farmers twice and then see if the answers match

* We could compare answers given to enumerators with third-party data, such as government census data. 

There are no easy answers to this question, and the solution is likely to be highly dependent on the question being asked and the country it is asked in. 

#### Pre-test, Pre-test and Pre-test again

Once we have a robust question, and a strategy to measure answers, we will want to **pre-test** the survey.

I cannot emphasize the importance of pre-testing questions enough. Pre-testing is the
process through which we draft questions, ask real farmers those questions and assess the extent to which the
question elicited the intended response. When pre-testing, we want to consider:

* Are the questions on the survey understood easily by farmers? 

* Are the questions interpreted the same way every time? e.g. "How much fertilizer do you use?" can be answered in terms of Kgs or money. We want to make sure we only get one type of answer!

* Do we want to provide limits on acceptable answers? e.g. "How many acres do you own?" with an answer of "100" is likely to be an error. We can set limits on answers to prevent these sorts of errors. 

Pre-testing allows us to identify questions that seem clear to us as analysts but are understood completely differently by farmers. Pre-testing will also allow us to derive a list of responses that is comprehensive. So if we ask:

* "What livestock do you keep?"

We can make sure that the options (cows, pigs, goats, chickens, other) are available to minimize the amount of "other" that we get.

The final key to great measurement is knowing how the data will be collected and stored. Will you send out paper surveys? If so what happens to the surveys after data entry, and how do we double-check data entry? Once we have the electronic data, where will it be stored so that it is accessible to future One Acre Fund staff? If you are collecting data on a tablet, will you use [CommCare](https://www.commcarehq.org)? 

At a minimum, I would expect all data to be well organized and available on the One Acre Fund Google drive folder. Likewise I would expect all analysis scripts to be deposited in the same folder and clearly linked to the raw data (we will cover this more in future lessons). 

> Summary

* Think about how to get robust and reliable data. Think about how to make sure we can trust the data.

* Pre-test surveys to find bugs and errors! Even if you think it is perfect, pre-test!

* Have a clear plan for where to store raw data and scripts. We need this to be accessible to other One Acre Fund analysts to preserve your work for future staff!


### Data checking

During the course of the RCT it is important to check data as it comes in. This will enable you to identify any issues early on and (hopefully) fix them before they become chronic. During data checking we are looking to see whether our survey is capturing the data expected. Some One Acre Fund examples of where this has been useful:

* Plot measurements by enumerators showing a total acreage of 0.001 acres

* Questions abour fertiliser use being answered both in kilograms and Kenyan Shillings.

In these cases we are really enquiring about **outliers**.

An outlier is a data point which lies outside the main distribution and may signal data recorded or entered incorrectly. The problem with outliers is trying to distinguish real data that is just unexpected (but is valid) and data which is incorrect due to enumerator survey error. 

We can visualize data with a beeswarm plot, note the potential outliers at the top of the plot:

```{r outl}
set.seed(112)
#make some data
df = data.frame(ID=seq(1,100), vals=rnorm(100,100,100))
#add some outliers
df$vals[98:100] = rnorm(3,1000,1000)


library(ggbeeswarm)
ggplot(df) +  geom_beeswarm(aes(x=1,y=vals)) + ggtitle("Beeswarm of simulated data") 



```

We can also try to define outliers mathematically, the boxplot uses a simple formula to identify potential outliers, I have included a function below that will use this logic to find outliers for numerical values:

```{r out2}

#this function expects a vector of numeric values
find_outliers <- function(data_to_test){

      IQR<- quantile(data_to_test, 0.75, na.rm=TRUE)[[1]] - quantile(data_to_test, 0.25, na.rm=TRUE)[[1]]
      cutoff = subset(data_to_test, data_to_test <=  quantile(data_to_test, 0.25, na.rm=TRUE)-(IQR*1.5) )  
      cutoff2 = subset(data_to_test, data_to_test >=   quantile(data_to_test, 0.75, na.rm=TRUE)+(IQR*1.5) ) 
      ret = c(cutoff,cutoff2)
      return(ret)
}

find_outliers(df$vals)


```


These tools can help us identify potential outliers but cannot definitively tell us whether a data point is real or not. When we assign something as an outlier we are making a big assumption about the underlying distribution of data and what *we* think it should look like. There is always a danger that our assumption is wrong or that we have biases leading us to incorrectly label things as outliers.

If we are confident that a value is mistaken (and not just a true extreme) then we can replace that value with NA, to indicate that it is missing. This will mean subsequent calculations will be done without that value. 

There is no easy solution to this, and often it will be contextual knowledge which we use to identify real outliers, for example, it is likely that a farmer reporting 10000 Kg of fertilizer per acre is actually a case of enumerator error. You can read more about outliers [here](https://drive.google.com/file/d/0Bz09wJM9mJC_M2kwZUIzSmFzNlE/view)

> Summary

* Data checking ensures that come analysis time, our data is free from outliers and issues

* Outliers are often difficult to define - remember the 68-95-99.7 rule from lesson 1. In a normal distribution of values we expect 68% of the values to be within 1 stdev of the mean. 95% to be within 2 stdev and 99.7% to be within 3 stdev of the mean. This means we can have fairly extreme values that are still well within what we expect for a normal distribution. 

* 

# Lesson Summary

We have now seen the key parts of an RCT:

* **Randomization**: We must randomize using R and ensuring our Treatment and Control groups are balanced. We can use my RCT_random function to achieve this. We must also remember cluster effects and use the correct *unit of randomization*. It is possible to calculate the ICC and adjust sample size accordingly, it is also fair to assume an ICC of 1.0 and have the sample size calculated from the number of clusters (i.e. calculate the number of groups rather than individuals).

* **Hypothesis**: A good hypothesis is *testable* and *measurable*. It must have a clearly defined evaluation criteria (e.g. are we measuring the average or the median? At the individual or group level?). 

* **Power**: Know your statistical power and sample size before and after a trial - see [AMP lesson 1](https://michael-bar.github.io/AMP-1-distributions/AMP_distributions_lesson.html#). We will need an estimate of effect size from historic data or peer-reviewed literature. 

* **Measurement**: Measurements must be robust and reliable, think about the ways that we might be inaccurate and try to minimize the important of self-reported metrics in any study you design!

* **Pre-test**: Pre-test all questions with real farmers to understand how *they* understand your survey and look for difficulties. Remember - a poor survey is due to the analysts bad communication, not the farmers bad interpretation! 

* Combining lesson 1 (distributions, powers, p-values) and lesson 2 (RCT principles) should now mean you are able to start designing RCTs! Our next few lessons will look at how to analyse data effectively using hypothesis testing and regressions. 


# Optional reading

* [The RCT vs. other trials at One Acre Fund](https://docs.google.com/presentation/d/14CpKbrEtFOzPfYTmfwKZgnpmANYbKC8YmX1ZxTRZsQs/edit#slide=id.p4)

* [Effective Hypothesis design](https://www.producttalk.org/2014/11/the-5-components-of-a-good-hypothesis)

* [Self-reporting pitfalls (reccomended!)](http://www.sciencebrainwaves.com/the-dangers-of-self-report/)



